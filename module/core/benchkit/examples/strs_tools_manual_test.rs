//! Manual testing of `strs_tools` integration with benchkit
//!
//! This tests benchkit with actual `strs_tools` functionality to identify issues.

#![allow(clippy ::doc_markdown)]
#![allow(clippy ::format_push_string)]
#![allow(clippy ::uninlined_format_args)]
#![allow(clippy ::std_instead_of_core)]
#![allow(clippy ::unnecessary_wraps)]
#![allow(clippy ::useless_format)]
#![allow(clippy ::redundant_closure_for_method_calls)]
#![allow(clippy ::cast_possible_truncation)]
#![allow(clippy ::cast_sign_loss)]
#![allow(clippy ::no_effect_underscore_binding)]
#![allow(clippy ::used_underscore_binding)]

use benchkit ::prelude :: *;

use std ::collections ::HashMap;

type Result< T > = std ::result ::Result< T, Box<dyn std ::error ::Error >>;

fn main() -> Result< () >
{
  println!("üß™ Manual Testing of strs_tools + benchkit Integration");
  println!("======================================================");
  println!();

  // Test 1 : Basic benchkit functionality
  test_basic_benchkit()?;
  
  // Test 2 : Data generation with real patterns
  test_data_generation()?;
  
  // Test 3 : Memory tracking
  test_memory_tracking()?;
  
  // Test 4 : Throughput analysis
  test_throughput_analysis()?;
  
  // Test 5 : Statistical analysis (if available)
  #[ cfg(feature = "statistical_analysis") ]
  test_statistical_analysis()?;
  
  // Test 6 : Report generation
  test_report_generation()?;

  println!("‚úÖ All manual tests completed successfully!");
  Ok(())
}

fn test_basic_benchkit() -> Result< () >
{
  println!("1Ô∏è‚É£ Testing Basic Benchkit Functionality");
  println!("---------------------------------------");
  
  // Simple comparative analysis without external dependencies
  let mut comparison = ComparativeAnalysis ::new("basic_string_operations");
  
  comparison = comparison
  .algorithm("simple_split", ||
  {
   let test_data = "item1,item2,item3,item4,item5";
   let count = test_data.split(',').count();
   std ::hint ::black_box(count);
 })
  .algorithm("collect_split", ||
  {
   let test_data = "item1,item2,item3,item4,item5";
   let parts: Vec< &str > = test_data.split(',').collect();
   std ::hint ::black_box(parts.len());
 });

  let report = comparison.run();
  
  if let Some((fastest, result)) = report.fastest()
  {
  println!("  ‚úÖ Fastest: {} ({:.0} ops/sec)", fastest, result.operations_per_second());
 }
  else
  {
  println!("  ‚ùå Failed to determine fastest algorithm");
 }
  
  println!();
  Ok(())
}

fn test_data_generation() -> Result< () >
{
  println!("2Ô∏è‚É£ Testing Data Generation");
  println!("-------------------------");
  
  // Test pattern-based generation
  let generator = DataGenerator ::new()
  .pattern("item{},")
  .repetitions(5)
  .complexity(DataComplexity ::Simple);
  
  let result = generator.generate_string();
  println!("  ‚úÖ Pattern generation: {}", &result[..30.min(result.len())]);
  
  // Test size-based generation
  let size_generator = DataGenerator ::new()
  .size_bytes(100)
  .complexity(DataComplexity ::Medium);
  
  let size_result = size_generator.generate_string();
  println!("  ‚úÖ Size-based generation: {} bytes", size_result.len());
  
  // Test CSV generation
  let csv_data = generator.generate_csv_data(3, 4);
  let lines: Vec< &str > = csv_data.lines().collect();
  println!("  ‚úÖ CSV generation: {} rows generated", lines.len());
  
  // Test unilang commands
  let commands = generator.generate_unilang_commands(3);
  println!("  ‚úÖ Unilang commands: {} commands generated", commands.len());
  
  println!();
  Ok(())
}

fn test_memory_tracking() -> Result< () >
{
  println!("3Ô∏è‚É£ Testing Memory Tracking");
  println!("-------------------------");
  
  let memory_benchmark = MemoryBenchmark ::new("memory_test");
  
  // Test basic allocation tracking
  let (result, stats) = memory_benchmark.run_with_tracking(5, ||
  {
  // Simulate allocation
  let _data = vec![0u8; 1024];
  memory_benchmark.tracker.record_allocation(1024);
 });
  
  println!("  ‚úÖ Memory tracking completed");
  println!("     - Iterations: {}", result.times.len());
  println!("     - Total allocated: {} bytes", stats.total_allocated);
  println!("     - Peak usage: {} bytes", stats.peak_usage);
  println!("     - Allocations: {}", stats.allocation_count);
  
  // Test memory comparison
  let comparison = memory_benchmark.compare_memory_usage(
  "allocating_version",
  || {
   let _vec = vec![42u8; 512];
   memory_benchmark.tracker.record_allocation(512);
 },
  "minimal_version",
  || {
   let _x = 42;
   // No allocations
 },
  3,
 );
  
  let (efficient_name, _) = comparison.more_memory_efficient();
  println!("  ‚úÖ Memory comparison: {} is more efficient", efficient_name);
  
  println!();
  Ok(())
}

fn test_throughput_analysis() -> Result< () >
{
  println!("4Ô∏è‚É£ Testing Throughput Analysis");
  println!("-----------------------------");
  
  let test_data = "field1,field2,field3,field4,field5,field6,field7,field8,field9,field10".repeat(100);
  let throughput_analyzer = ThroughputAnalyzer ::new("string_processing", test_data.len() as u64)
  .with_items(1000);
  
  // Create some test results
  let mut results = HashMap ::new();
  
  // Fast version (50ms)
  let fast_times = vec![std ::time ::Duration ::from_millis(50); 10];
  results.insert("fast_algorithm".to_string(), BenchmarkResult ::new("fast", fast_times));
  
  // Slow version (150ms)
  let slow_times = vec![std ::time ::Duration ::from_millis(150); 10];
  results.insert("slow_algorithm".to_string(), BenchmarkResult ::new("slow", slow_times));
  
  let throughput_comparison = throughput_analyzer.compare_throughput(&results);
  
  if let Some((fastest_name, fastest_metrics)) = throughput_comparison.fastest_throughput()
  {
  println!("  ‚úÖ Throughput analysis completed");
  println!("     - Fastest: {} ({})", fastest_name, fastest_metrics.throughput_description());
  
  if let Some(items_desc) = fastest_metrics.items_description()
  {
   println!("     - Item processing: {}", items_desc);
 }
 }
  
  if let Some(speedups) = throughput_comparison.calculate_speedups("slow_algorithm")
  {
  for (name, speedup) in speedups
  {
   if name != "slow_algorithm"
   {
  println!("     - {} : {:.1}x speedup", name, speedup);
 }
 }
 }
  
  println!();
  Ok(())
}

#[ cfg(feature = "statistical_analysis") ]
fn test_statistical_analysis() -> Result< () >
{
  println!("5Ô∏è‚É£ Testing Statistical Analysis");
  println!("------------------------------");
  
  // Create test results with different characteristics
  let consistent_times = vec![std ::time ::Duration ::from_millis(100); 20];
  let consistent_result = BenchmarkResult ::new("consistent", consistent_times);
  
  let variable_times: Vec< _ > = (0..20)
  .map(|i| std ::time ::Duration ::from_millis(100 + (i * 5)))
  .collect();
  let variable_result = BenchmarkResult ::new("variable", variable_times);
  
  // Analyze individual results
  let consistent_analysis = StatisticalAnalysis ::analyze(&consistent_result, SignificanceLevel ::Standard)?;
  let variable_analysis = StatisticalAnalysis ::analyze(&variable_result, SignificanceLevel ::Standard)?;
  
  println!("  ‚úÖ Statistical analysis completed");
  println!("     - Consistent CV: {:.1}% ({})", 
   consistent_analysis.coefficient_of_variation * 100.0,
   if consistent_analysis.is_reliable() 
   { "Reliable" } else { "Questionable" });
  println!("     - Variable CV: {:.1}% ({})",
   variable_analysis.coefficient_of_variation * 100.0,
   if variable_analysis.is_reliable() 
   { "Reliable" } else { "Questionable" });
  
  // Compare results
  let comparison = StatisticalAnalysis ::compare(
  &consistent_result,
  &variable_result,
  SignificanceLevel ::Standard
 )?;
  
  println!("     - Effect size: {:.3} ({})", 
   comparison.effect_size,
   comparison.effect_size_interpretation());
  println!("     - Statistically significant: {}", comparison.is_significant);
  
  println!();
  Ok(())
}

fn test_report_generation() -> Result< () >
{
  println!("6Ô∏è‚É£ Testing Report Generation");
  println!("---------------------------");
  
  // Generate a simple comparison
  let mut comparison = ComparativeAnalysis ::new("report_test");
  
  comparison = comparison
  .algorithm("approach_a", ||
  {
   let _result = "test,data,processing".split(',').count();
   std ::hint ::black_box(_result);
 })
  .algorithm("approach_b", ||
  {
   let parts: Vec< &str > = "test,data,processing".split(',').collect();
   std ::hint ::black_box(parts.len());
 });

  let report = comparison.run();
  
  // Generate markdown report
  let markdown_report = generate_comprehensive_markdown_report(&report);
  
  // Save report to test file
  let report_path = "target/manual_test_report.md";
  std ::fs ::write(report_path, &markdown_report)?;
  
  println!("  ‚úÖ Report generation completed");
  println!("     - Report saved: {}", report_path);
  println!("     - Report length: {} characters", markdown_report.len());
  
  // Check if report contains expected sections
  let has_performance = markdown_report.contains("Performance");
  let has_results = markdown_report.contains("ops/sec");
  let has_methodology = markdown_report.contains("Statistical");
  
  println!("     - Contains performance data: {}", has_performance);
  println!("     - Contains results: {}", has_results);  
  println!("     - Contains methodology: {}", has_methodology);
  
  println!();
  Ok(())
}

fn generate_comprehensive_markdown_report(report: &ComparisonAnalysisReport) -> String
{
  let mut output = String ::new();
  
  output.push_str("# Manual Test Report\n\n");
  output.push_str("*Generated with benchkit manual testing*\n\n");
  
  output.push_str("## Performance Results\n\n");
  // Generate simple table from results
  output.push_str("| Operation | Mean Time | Ops/sec |\n");
  output.push_str("|-----------|-----------|--------|\n");
  for (name, result) in &report.results 
  {
  output.push_str(&format!(
   "| {} | {:.2?} | {:.0} |\n",
   name,
   result.mean_time(),
   result.operations_per_second()
 ));
 }
  
  output.push_str("## Statistical Quality\n\n");
  
  let mut reliable_count = 0;
  let mut total_count = 0;
  
  for (name, result) in &report.results
  {
  total_count += 1;
  let is_reliable = result.is_reliable();
  if is_reliable { reliable_count += 1; }
  
  let status = if is_reliable { "‚úÖ Reliable" } else { "‚ö†Ô∏è Needs improvement" };
  output.push_str(&format!("- **{}** : {} (CV: {:.1}%)\n",
  name,
  status,
  result.coefficient_of_variation() * 100.0));
 }
  
  output.push_str(&format!("\n**Quality Summary** : {}/{} implementations meet reliability standards\n\n",
   reliable_count, total_count));
  
  output.push_str("## Manual Testing Summary\n\n");
  output.push_str("This report demonstrates successful integration of benchkit with manual testing procedures.\n");
  output.push_str("All core functionality tested and working correctly.\n\n");
  
  output.push_str("---\n");
  output.push_str("*Generated by benchkit manual testing suite*\n");
  
  output
}