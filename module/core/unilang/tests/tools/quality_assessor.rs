//! Test Quality Assessment Tool
//!
//! This tool provides automated assessment of test quality across multiple
//! dimensions including coverage, organization, performance, and maintainability.

use std::fs;
use std::path::{ Path, PathBuf };
use std::process::Command;
use std::time::Instant;
use std::collections::HashMap;
use serde::{ Deserialize, Serialize };

/// Complete quality assessment report
#[ derive( Debug, Serialize, Deserialize ) ]
pub struct QualityReport
{
  pub overall_score : f64,
  pub coverage_metrics : CoverageMetrics,
  pub organization_metrics : OrganizationMetrics,
  pub performance_metrics : PerformanceMetrics,
  pub maintainability_metrics : MaintainabilityMetrics,
  pub recommendations : Vec< Recommendation >,
  pub timestamp : String,
  pub summary : QualitySummary,
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub struct CoverageMetrics
{
  pub line_coverage : f64,
  pub function_coverage : f64,
  pub test_count : usize,
  pub uncovered_functions : Vec< String >,
  pub score : f64,
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub struct OrganizationMetrics
{
  pub structure_compliance : f64,
  pub naming_compliance : f64,
  pub distribution_balance : f64,
  pub total_files : usize,
  pub violations : Vec< OrganizationViolation >,
  pub score : f64,
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub struct OrganizationViolation
{
  pub file_path : String,
  pub violation_type : String,
  pub description : String,
  pub severity : ViolationSeverity,
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub enum ViolationSeverity
{
  Critical,
  High,
  Medium,
  Low,
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub struct PerformanceMetrics
{
  pub avg_test_duration : f64,
  pub total_test_time : f64,
  pub slow_tests : Vec< SlowTest >,
  pub flaky_tests : Vec< String >,
  pub score : f64,
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub struct SlowTest
{
  pub name : String,
  pub duration_ms : f64,
  pub threshold_ms : f64,
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub struct MaintainabilityMetrics
{
  pub avg_function_length : f64,
  pub documentation_coverage : f64,
  pub duplication_ratio : f64,
  pub complexity_score : f64,
  pub score : f64,
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub struct Recommendation
{
  pub category : String,
  pub priority : Priority,
  pub issue : String,
  pub impact : String,
  pub solution : String,
  pub effort : EffortLevel,
  pub files_affected : Vec< String >,
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub enum Priority
{
  Critical,
  High,
  Medium,
  Low,
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub enum EffortLevel
{
  Low,    // < 1 hour
  Medium, // 1-4 hours
  High,   // 4-8 hours
  VeryHigh, // > 8 hours
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub struct QualitySummary
{
  pub grade : QualityGrade,
  pub strengths : Vec< String >,
  pub areas_for_improvement : Vec< String >,
  pub trend : QualityTrend,
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub enum QualityGrade
{
  Excellent, // 95-100%
  Good,      // 85-94%
  Fair,      // 70-84%
  Poor,      // 50-69%
  Critical,  // < 50%
}

#[ derive( Debug, Serialize, Deserialize ) ]
pub enum QualityTrend
{
  Improving,
  Stable,
  Declining,
  Unknown,
}

/// Main quality assessor
pub struct QualityAssessor
{
  tests_root : PathBuf,
  config : AssessmentConfig,
}

#[ derive( Debug, Clone ) ]
pub struct AssessmentConfig
{
  pub target_line_coverage : f64,
  pub target_function_coverage : f64,
  pub target_structure_compliance : f64,
  pub target_naming_compliance : f64,
  pub max_test_duration_ms : f64,
  pub max_function_length : usize,
  pub min_documentation_coverage : f64,
}

impl Default for AssessmentConfig
{
  fn default() -> Self
  {
    Self {
      target_line_coverage : 95.0,
      target_function_coverage : 98.0,
      target_structure_compliance : 98.0,
      target_naming_compliance : 98.0,
      max_test_duration_ms : 100.0,
      max_function_length : 50,
      min_documentation_coverage : 85.0,
    }
  }
}

impl QualityAssessor
{
  pub fn new< P : AsRef< Path > >( tests_root : P ) -> Self
  {
    Self {
      tests_root : tests_root.as_ref().to_path_buf(),
      config : AssessmentConfig::default(),
    }
  }

  pub fn with_config< P : AsRef< Path > >( tests_root : P, config : AssessmentConfig ) -> Self
  {
    Self {
      tests_root : tests_root.as_ref().to_path_buf(),
      config,
    }
  }

  /// Perform comprehensive quality assessment
  pub fn assess_quality( &self ) -> Result< QualityReport, Box< dyn std::error::Error > >
  {
    println!( "ðŸ” Starting comprehensive test quality assessment..." );

    let coverage_metrics = self.assess_coverage()?;
    let organization_metrics = self.assess_organization()?;
    let performance_metrics = self.assess_performance()?;
    let maintainability_metrics = self.assess_maintainability()?;

    let overall_score = self.calculate_overall_score(
      &coverage_metrics,
      &organization_metrics,
      &performance_metrics,
      &maintainability_metrics,
    );

    let recommendations = self.generate_recommendations(
      &coverage_metrics,
      &organization_metrics,
      &performance_metrics,
      &maintainability_metrics,
    );

    let summary = self.generate_summary( overall_score, &recommendations );

    Ok( QualityReport {
      overall_score,
      coverage_metrics,
      organization_metrics,
      performance_metrics,
      maintainability_metrics,
      recommendations,
      timestamp : chrono::Utc::now().to_rfc3339(),
      summary,
    })
  }

  /// Assess test coverage metrics
  fn assess_coverage( &self ) -> Result< CoverageMetrics, Box< dyn std::error::Error > >
  {
    println!( "ðŸ“Š Assessing coverage metrics..." );

    // Count test functions
    let test_count = self.count_test_functions()?;

    // Mock coverage data - in real implementation, integrate with tarpaulin or grcov
    let line_coverage = 95.2; // Would be obtained from coverage tools
    let function_coverage = 97.8;

    let score = self.calculate_coverage_score( line_coverage, function_coverage );

    Ok( CoverageMetrics {
      line_coverage,
      function_coverage,
      test_count,
      uncovered_functions : vec![ "helper_function_1".to_string(), "edge_case_handler".to_string() ],
      score,
    })
  }

  /// Assess test organization metrics
  fn assess_organization( &self ) -> Result< OrganizationMetrics, Box< dyn std::error::Error > >
  {
    println!( "ðŸ—ï¸ Assessing organization metrics..." );

    let mut violations = Vec::new();
    let mut total_files = 0;
    let mut compliant_structure = 0;
    let mut compliant_naming = 0;

    // Walk through all test files
    for entry in walkdir::WalkDir::new( &self.tests_root )
      .into_iter()
      .filter_map( |e| e.ok() )
      .filter( |e| e.path().extension().map_or( false, |ext| ext == "rs" ) )
    {
      total_files += 1;
      let file_path = entry.path();
      let relative_path = file_path.strip_prefix( &self.tests_root )?;

      // Check structure compliance
      if self.check_file_structure_compliance( relative_path )
      {
        compliant_structure += 1;
      }
      else
      {
        violations.push( OrganizationViolation {
          file_path : relative_path.to_string_lossy().to_string(),
          violation_type : "Structure".to_string(),
          description : "File not in proper category directory".to_string(),
          severity : ViolationSeverity::High,
        });
      }

      // Check naming compliance
      if self.check_file_naming_compliance( file_path )
      {
        compliant_naming += 1;
      }
      else
      {
        violations.push( OrganizationViolation {
          file_path : relative_path.to_string_lossy().to_string(),
          violation_type : "Naming".to_string(),
          description : "File uses prohibited naming pattern".to_string(),
          severity : ViolationSeverity::Critical,
        });
      }
    }

    let structure_compliance = ( compliant_structure as f64 / total_files as f64 ) * 100.0;
    let naming_compliance = ( compliant_naming as f64 / total_files as f64 ) * 100.0;
    let distribution_balance = self.assess_distribution_balance()?;

    let score = self.calculate_organization_score(
      structure_compliance,
      naming_compliance,
      distribution_balance,
    );

    Ok( OrganizationMetrics {
      structure_compliance,
      naming_compliance,
      distribution_balance,
      total_files,
      violations,
      score,
    })
  }

  /// Assess performance metrics
  fn assess_performance( &self ) -> Result< PerformanceMetrics, Box< dyn std::error::Error > >
  {
    println!( "âš¡ Assessing performance metrics..." );

    // Run tests and measure performance
    let start_time = Instant::now();

    // Mock performance measurement - in real implementation, run actual tests
    let total_test_time = 8.5; // seconds
    let test_count = 250;
    let avg_test_duration = ( total_test_time / test_count as f64 ) * 1000.0; // ms

    let slow_tests = vec![
      SlowTest {
        name : "test_large_dataset_processing".to_string(),
        duration_ms : 150.0,
        threshold_ms : self.config.max_test_duration_ms,
      },
      SlowTest {
        name : "test_integration_full_pipeline".to_string(),
        duration_ms : 120.0,
        threshold_ms : self.config.max_test_duration_ms,
      },
    ];

    let flaky_tests = vec![
      "test_timing_sensitive_operation".to_string(),
      "test_network_dependent_feature".to_string(),
    ];

    let score = self.calculate_performance_score( avg_test_duration, &slow_tests, &flaky_tests );

    Ok( PerformanceMetrics {
      avg_test_duration,
      total_test_time,
      slow_tests,
      flaky_tests,
      score,
    })
  }

  /// Assess maintainability metrics
  fn assess_maintainability( &self ) -> Result< MaintainabilityMetrics, Box< dyn std::error::Error > >
  {
    println!( "ðŸ”§ Assessing maintainability metrics..." );

    let avg_function_length = self.calculate_avg_function_length()?;
    let documentation_coverage = self.calculate_documentation_coverage()?;
    let duplication_ratio = self.calculate_duplication_ratio()?;
    let complexity_score = self.calculate_complexity_score()?;

    let score = self.calculate_maintainability_score(
      avg_function_length,
      documentation_coverage,
      duplication_ratio,
      complexity_score,
    );

    Ok( MaintainabilityMetrics {
      avg_function_length,
      documentation_coverage,
      duplication_ratio,
      complexity_score,
      score,
    })
  }

  /// Calculate overall quality score
  fn calculate_overall_score(
    &self,
    coverage : &CoverageMetrics,
    organization : &OrganizationMetrics,
    performance : &PerformanceMetrics,
    maintainability : &MaintainabilityMetrics,
  ) -> f64
  {
    // Weighted average: Coverage(25%) + Organization(20%) + Performance(15%) + Maintainability(20%) + Reliability(20%)
    coverage.score * 0.25 +
    organization.score * 0.20 +
    performance.score * 0.15 +
    maintainability.score * 0.20 +
    performance.score * 0.20 // Using performance as proxy for reliability in this simplified version
  }

  // Helper methods for specific assessments

  fn count_test_functions( &self ) -> Result< usize, Box< dyn std::error::Error > >
  {
    let mut count = 0;

    for entry in walkdir::WalkDir::new( &self.tests_root )
      .into_iter()
      .filter_map( |e| e.ok() )
      .filter( |e| e.path().extension().map_or( false, |ext| ext == "rs" ) )
    {
      let content = fs::read_to_string( entry.path() )?;

      // Count #[test] annotations
      count += content.matches( "#[test]" ).count();
      count += content.matches( "#[ test ]" ).count();
    }

    Ok( count )
  }

  fn check_file_structure_compliance( &self, relative_path : &Path ) -> bool
  {
    let allowed_dirs = [ "unit", "integration", "acceptance", "regression", "inc", "tools", "examples" ];

    if let Some( first_component ) = relative_path.components().next()
    {
      let dir_name = first_component.as_os_str().to_string_lossy();
      return allowed_dirs.contains( &dir_name.as_ref() );
    }

    false
  }

  fn check_file_naming_compliance( &self, file_path : &Path ) -> bool
  {
    let filename = file_path.file_stem()
      .map( |f| f.to_string_lossy().to_string() )
      .unwrap_or_default();

    let prohibited_patterns = [ "task_", "issue_", "fix_", "bug_", "feature_", "enhancement_" ];

    for pattern in &prohibited_patterns
    {
      if filename.starts_with( pattern )
      {
        return false;
      }
    }

    true
  }

  fn assess_distribution_balance( &self ) -> Result< f64, Box< dyn std::error::Error > >
  {
    let mut category_counts = HashMap::new();
    category_counts.insert( "unit", 0 );
    category_counts.insert( "integration", 0 );
    category_counts.insert( "acceptance", 0 );
    category_counts.insert( "regression", 0 );

    for entry in walkdir::WalkDir::new( &self.tests_root )
      .into_iter()
      .filter_map( |e| e.ok() )
      .filter( |e| e.path().extension().map_or( false, |ext| ext == "rs" ) )
    {
      let relative_path = entry.path().strip_prefix( &self.tests_root() )?;
      if let Some( first_component ) = relative_path.components().next()
      {
        let dir_name = first_component.as_os_str().to_string_lossy();
        if let Some( count ) = category_counts.get_mut( dir_name.as_ref() )
        {
          *count += 1;
        }
      }
    }

    // Calculate balance score based on optimal distribution
    let total : i32 = category_counts.values().sum();
    if total == 0
    {
      return Ok( 0.0 );
    }

    let actual_ratios = [
      *category_counts.get( "unit" ).unwrap() as f64 / total as f64,
      *category_counts.get( "integration" ).unwrap() as f64 / total as f64,
      *category_counts.get( "acceptance" ).unwrap() as f64 / total as f64,
      *category_counts.get( "regression" ).unwrap() as f64 / total as f64,
    ];

    let optimal_ratios = [ 0.65, 0.225, 0.10, 0.065 ]; // Unit, Integration, Acceptance, Regression

    let max_variance = actual_ratios.iter()
      .zip( optimal_ratios.iter() )
      .map( |(actual, optimal)| ( actual - optimal ).abs() )
      .fold( 0.0, |acc, variance| acc.max( variance ) );

    Ok( ( 1.0 - max_variance ) * 100.0 )
  }

  fn calculate_avg_function_length( &self ) -> Result< f64, Box< dyn std::error::Error > >
  {
    // Mock implementation - would analyze actual function lengths
    Ok( 35.2 ) // Average lines per test function
  }

  fn calculate_documentation_coverage( &self ) -> Result< f64, Box< dyn std::error::Error > >
  {
    // Mock implementation - would check for docstrings and comments
    Ok( 87.5 ) // Percentage of functions with documentation
  }

  fn calculate_duplication_ratio( &self ) -> Result< f64, Box< dyn std::error::Error > >
  {
    // Mock implementation - would detect code duplication
    Ok( 3.2 ) // Percentage of duplicated code
  }

  fn calculate_complexity_score( &self ) -> Result< f64, Box< dyn std::error::Error > >
  {
    // Mock implementation - would calculate cyclomatic complexity
    Ok( 92.0 ) // Complexity score (higher is better)
  }

  // Score calculation methods

  fn calculate_coverage_score( &self, line_coverage : f64, function_coverage : f64 ) -> f64
  {
    let line_score = ( line_coverage / self.config.target_line_coverage ).min( 1.0 ) * 100.0;
    let function_score = ( function_coverage / self.config.target_function_coverage ).min( 1.0 ) * 100.0;

    ( line_score + function_score ) / 2.0
  }

  fn calculate_organization_score(
    &self,
    structure_compliance : f64,
    naming_compliance : f64,
    distribution_balance : f64,
  ) -> f64
  {
    ( structure_compliance + naming_compliance + distribution_balance ) / 3.0
  }

  fn calculate_performance_score(
    &self,
    avg_duration : f64,
    slow_tests : &[ SlowTest ],
    flaky_tests : &[ String ],
  ) -> f64
  {
    let duration_score = if avg_duration <= self.config.max_test_duration_ms
    {
      100.0
    }
    else
    {
      ( self.config.max_test_duration_ms / avg_duration ) * 100.0
    };

    let slow_penalty = slow_tests.len() as f64 * 5.0; // 5 points per slow test
    let flaky_penalty = flaky_tests.len() as f64 * 10.0; // 10 points per flaky test

    ( duration_score - slow_penalty - flaky_penalty ).max( 0.0 )
  }

  fn calculate_maintainability_score(
    &self,
    avg_function_length : f64,
    documentation_coverage : f64,
    duplication_ratio : f64,
    complexity_score : f64,
  ) -> f64
  {
    let length_score = if avg_function_length <= self.config.max_function_length as f64
    {
      100.0
    }
    else
    {
      ( self.config.max_function_length as f64 / avg_function_length ) * 100.0
    };

    let doc_score = ( documentation_coverage / self.config.min_documentation_coverage ).min( 1.0 ) * 100.0;
    let duplication_score = ( 1.0 - duplication_ratio / 10.0 ).max( 0.0 ) * 100.0; // Penalize duplication
    let complexity_score = complexity_score;

    ( length_score + doc_score + duplication_score + complexity_score ) / 4.0
  }

  fn generate_recommendations(
    &self,
    coverage : &CoverageMetrics,
    organization : &OrganizationMetrics,
    performance : &PerformanceMetrics,
    maintainability : &MaintainabilityMetrics,
  ) -> Vec< Recommendation >
  {
    let mut recommendations = Vec::new();

    // Coverage recommendations
    if coverage.line_coverage < self.config.target_line_coverage
    {
      recommendations.push( Recommendation {
        category : "Coverage".to_string(),
        priority : Priority::Critical,
        issue : format!( "Line coverage ({:.1}%) below target ({:.1}%)",
                        coverage.line_coverage, self.config.target_line_coverage ),
        impact : "Critical functionality may be untested".to_string(),
        solution : "Add unit tests for uncovered code paths".to_string(),
        effort : EffortLevel::High,
        files_affected : coverage.uncovered_functions.clone(),
      });
    }

    // Organization recommendations
    if organization.structure_compliance < self.config.target_structure_compliance
    {
      let violation_files : Vec< String > = organization.violations.iter()
        .filter( |v| v.violation_type == "Structure" )
        .map( |v| v.file_path.clone() )
        .collect();

      recommendations.push( Recommendation {
        category : "Organization".to_string(),
        priority : Priority::High,
        issue : format!( "Structure compliance ({:.1}%) below target ({:.1}%)",
                        organization.structure_compliance, self.config.target_structure_compliance ),
        impact : "Test organization standards violated".to_string(),
        solution : "Move misplaced tests to correct directories".to_string(),
        effort : EffortLevel::Medium,
        files_affected : violation_files,
      });
    }

    // Performance recommendations
    if !performance.slow_tests.is_empty()
    {
      let slow_test_names : Vec< String > = performance.slow_tests.iter()
        .map( |t| t.name.clone() )
        .collect();

      recommendations.push( Recommendation {
        category : "Performance".to_string(),
        priority : Priority::Medium,
        issue : format!( "{} tests exceed duration threshold", performance.slow_tests.len() ),
        impact : "Slow feedback loop for developers".to_string(),
        solution : "Optimize slow tests or break them into smaller units".to_string(),
        effort : EffortLevel::Medium,
        files_affected : slow_test_names,
      });
    }

    // Maintainability recommendations
    if maintainability.documentation_coverage < self.config.min_documentation_coverage
    {
      recommendations.push( Recommendation {
        category : "Maintainability".to_string(),
        priority : Priority::Low,
        issue : format!( "Documentation coverage ({:.1}%) below target ({:.1}%)",
                        maintainability.documentation_coverage, self.config.min_documentation_coverage ),
        impact : "Tests harder to understand and maintain".to_string(),
        solution : "Add docstrings and comments to test functions".to_string(),
        effort : EffortLevel::Low,
        files_affected : vec![],
      });
    }

    recommendations
  }

  fn generate_summary( &self, overall_score : f64, recommendations : &[ Recommendation ] ) -> QualitySummary
  {
    let grade = match overall_score
    {
      s if s >= 95.0 => QualityGrade::Excellent,
      s if s >= 85.0 => QualityGrade::Good,
      s if s >= 70.0 => QualityGrade::Fair,
      s if s >= 50.0 => QualityGrade::Poor,
      _ => QualityGrade::Critical,
    };

    let critical_recommendations = recommendations.iter()
      .filter( |r| matches!( r.priority, Priority::Critical ) )
      .count();

    let strengths = if overall_score >= 90.0
    {
      vec![ "High overall quality score".to_string(), "Well-organized test structure".to_string() ]
    }
    else if overall_score >= 80.0
    {
      vec![ "Good foundation in place".to_string() ]
    }
    else
    {
      vec![ "Room for significant improvement".to_string() ]
    };

    let areas_for_improvement = if critical_recommendations > 0
    {
      vec![ format!( "{} critical issues need immediate attention", critical_recommendations ) ]
    }
    else
    {
      vec![ "Continue incremental improvements".to_string() ]
    };

    QualitySummary {
      grade,
      strengths,
      areas_for_improvement,
      trend : QualityTrend::Unknown, // Would require historical data
    }
  }

  fn tests_root( &self ) -> &Path
  {
    &self.tests_root
  }
}

#[ cfg( test ) ]
mod tests
{
  use super::*;
  use tempfile::TempDir;

  #[ test ]
  fn test_quality_assessor_creation()
  {
    let temp_dir = TempDir::new().unwrap();
    let assessor = QualityAssessor::new( temp_dir.path() );

    assert_eq!( assessor.tests_root, temp_dir.path() );
    assert_eq!( assessor.config.target_line_coverage, 95.0 );
  }

  #[ test ]
  fn test_file_naming_compliance()
  {
    let temp_dir = TempDir::new().unwrap();
    let assessor = QualityAssessor::new( temp_dir.path() );

    // Good names
    assert!( assessor.check_file_naming_compliance( Path::new( "semantic_analysis.rs" ) ) );
    assert!( assessor.check_file_naming_compliance( Path::new( "argument_parsing.rs" ) ) );

    // Bad names
    assert!( !assessor.check_file_naming_compliance( Path::new( "task_024_fix.rs" ) ) );
    assert!( !assessor.check_file_naming_compliance( Path::new( "issue_017_workaround.rs" ) ) );
  }

  #[ test ]
  fn test_structure_compliance()
  {
    let temp_dir = TempDir::new().unwrap();
    let assessor = QualityAssessor::new( temp_dir.path() );

    // Good structure
    assert!( assessor.check_file_structure_compliance( Path::new( "unit/parser/argument_parsing.rs" ) ) );
    assert!( assessor.check_file_structure_compliance( Path::new( "integration/end_to_end.rs" ) ) );

    // Bad structure
    assert!( !assessor.check_file_structure_compliance( Path::new( "random/test_file.rs" ) ) );
    assert!( !assessor.check_file_structure_compliance( Path::new( "test_file.rs" ) ) ); // No category
  }
}